{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c431932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "846659b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neurology</td>\n",
       "      <td>CC:, Confusion and slurred speech.,HX , (prima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Urology</td>\n",
       "      <td>PROCEDURE: , Elective male sterilization via b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Urology</td>\n",
       "      <td>INDICATION:,  Prostate Cancer.,TECHNIQUE:,  3....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Urology</td>\n",
       "      <td>DESCRIPTION:,  The patient was placed in the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Urology</td>\n",
       "      <td>PREOPERATIVE DIAGNOSIS: , Voluntary sterility....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  medical_specialty                                      transcription\n",
       "0         Neurology  CC:, Confusion and slurred speech.,HX , (prima...\n",
       "1           Urology  PROCEDURE: , Elective male sterilization via b...\n",
       "2           Urology  INDICATION:,  Prostate Cancer.,TECHNIQUE:,  3....\n",
       "3           Urology  DESCRIPTION:,  The patient was placed in the s...\n",
       "4           Urology  PREOPERATIVE DIAGNOSIS: , Voluntary sterility...."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\Harshit Singh\\Music\\DSAI\\mtsamplesV2.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a8b1623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1239, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99877caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1239 entries, 0 to 1238\n",
      "Data columns (total 2 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   medical_specialty  1239 non-null   object\n",
      " 1   transcription      1231 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 19.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44cdbaaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "medical_specialty    0\n",
       "transcription        8\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40e29237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "medical_specialty    0\n",
       "transcription        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = df.dropna()\n",
    "new_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e79a22ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' Neurology', ' Urology', ' Radiology', ' Orthopedic',\n",
       "       ' Gastroenterology'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['medical_specialty'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e5a27f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Orthopedic          355\n",
       " Radiology           273\n",
       " Gastroenterology    224\n",
       " Neurology           223\n",
       " Urology             156\n",
       "Name: medical_specialty, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['medical_specialty'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8590f624",
   "metadata": {},
   "source": [
    "## Text Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71e3a974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample transcription 1:PREOPERATIVE DIAGNOSIS: , Voluntary sterility.,POSTOPERATIVE DIAGNOSIS: , Voluntary sterility.,OPERATIVE PROCEDURE:,  Bilateral vasectomy.,ANESTHESIA:,  Local.,INDICATIONS FOR PROCEDURE:  ,A gentleman who is here today requesting voluntary sterility.  Options were discussed for voluntary sterility and he has elected to proceed with a bilateral vasectomy.,DESCRIPTION OF PROCEDURE:  ,The patient was brought to the operating room, and after appropriately identifying the patient, the patient was prepped and draped in the standard surgical fashion and placed in a supine position on the OR table.  Then, 0.25% Marcaine without epinephrine was used to anesthetize the scrotal skin.  A small incision was made in the right hemiscrotum.  The vas deferens was grasped with a vas clamp.  Next, the vas deferens was skeletonized.  It was clipped proximally and distally twice.  The cut edges were fulgurated.  Meticulous hemostasis was maintained.  Then, 4-0 chromic was used to close the scrotal skin on the right hemiscrotum.  Next, the attention was turned to the left hemiscrotum, and after the left hemiscrotum was anesthetized appropriately, a small incision was made in the left hemiscrotum.  The vas deferens was isolated.  It was skeletonized.  It was clipped proximally and distally twice.  The cut edges were fulgurated.  Meticulous hemostasis was maintained.  Then, 4-0 chromic was used to close the scrotal skin.  A jockstrap and sterile dressing were applied at the end of the case.  Sponge, needle, and instruments counts were correct.\n",
      "\n",
      "Sample transcription 2:PROCEDURE PERFORMED: , Umbilical hernia repair.,PROCEDURE:,  After informed consent was obtained, the patient was brought to the operative suite and placed supine on the operating table.  The patient was sedated, and an adequate local anesthetic was administered using 1% lidocaine without epinephrine.  The patient was prepped and draped in the usual sterile manner.,A standard curvilinear umbilical incision was made, and dissection was carried down to the hernia sac using a combination of Metzenbaum scissors and Bovie electrocautery.  The sac was cleared of overlying adherent tissue, and the fascial defect was delineated.  The fascia was cleared of any adherent tissue for a distance of 1.5 cm from the defect.  The sac was then placed into the abdominal cavity and the defect was closed primarily using simple interrupted 0 Vicryl sutures.  The umbilicus was then re-formed using 4-0 Vicryl to tack the umbilical skin to the fascia.,The wound was then irrigated using sterile saline, and hemostasis was obtained using Bovie electrocautery.  The skin was approximated with 4-0 Vicryl in a subcuticular fashion.  The skin was prepped with benzoin, and Steri-Strips were applied.  A dressing was then applied.  All surgical counts were reported as correct.,Having tolerated the procedure well, the patient was subsequently taken to the recovery room in good and stable condition.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Sample transcription 1:'+df.iloc[4]['transcription']+'\\n')\n",
    "print('Sample transcription 2:'+df.iloc[14]['transcription']+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f44d2734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "special_character_remover = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "extra_symbol_remover = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2fb593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'\\d+', '', text) # Remove numbers\n",
    "    text = text.lower()\n",
    "    text = special_character_remover.sub(' ',text)\n",
    "    text = extra_symbol_remover.sub('',text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    return text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    wordlist=[]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    sentences=sent_tokenize(text)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words=word_tokenize(sentence)\n",
    "        for word in words:\n",
    "            wordlist.append(lemmatizer.lemmatize(word))    \n",
    "    return ' '.join(wordlist) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "208a1dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['transcription'] = df['transcription'].apply(clean_text)\n",
    "df['transcription'] = df['transcription'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66944ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Transcription 1:chief complaint\n",
      "\n",
      "Sample Transcription 2:preoperative diagnosis benign prostatic hypertrophy urinary retention postoperative diagnosis benign prostatic hypertrophy urinary retention procedure performed cystourethroscopy transurethral resection prostate turp anesthesia spinal drain # french threeway foley catheter specimen prostatic resection chip estimated blood loss cc disposition patient transferred pacu stable condition indication finding yearold male history bph subsequent urinary retention failure trial void scheduled elective turp procedure finding time surgery cystourethroscopy revealed trilobar enlargement prostate prostatic varix median lobe cystoscopy showed cellules bladder obvious bladder tumor noted description procedure informed consent obtained patient moved operating room spinal anesthesia induced department anesthesia patient prepped draped normal sterile fashion # french cystoscope inserted urethra bladder cystoscopy performed finding cystoscope removed # french resectoscope # cutting loop inserted bladder verumontanum identified landmark systematic transurethral resection prostate tissue undertaken circumferential fashion good resection tissue completed ________ irrigator used evacuate bladder prostatic chip resectoscope inserted residual chip removed piecemeal fashion resectoscope loop obvious bleeding prostatic fossa controlled electrocautery resectoscope removed # french threeway foley catheter inserted urethra bladder bladder irrigated connected threeway irrigation patient cleaned sent recovery stable condition admitted overnight continuous bladder irrigation postop monitoring\n",
      "\n",
      "Sample Transcription 3:cc right sided numbness hx male presented month history progressive right sided numbness anesthetic pain addition experienced worsening balance episode aspiration eating pmh born prematurely weighed # oz multiple episode aspiration pneumonia infant child asd repair age left ptosis repair age scoliosis gait abnormality poor pharyngeal reflex shx fhx mainstream high school education mental retardation ambulatory work cardboard shop disabled exam short stature head tilt right cn left ptosis decreased left nasolabial fold decreased gag reflex bilaterally motor full strength sensory marked hypesthesia entire right side coord slowed ram left station drift gait nd reflex + throughout babinski sign bilaterally beat ankle clonus right beat ankle clonus left mri arnold chiari ii syrinx severe basilar invagination marked compression ventral pontomedullary junction downward descension cerebellar tonsil vermis course patient underwent transpalatal pharyngeal ventral decompression pons medulla resection clivus odontoid tracheostomy placement halo vest ring removed month later philadelphia collar removed last seen mildly spastic gait good strength hyperreflexia throughout gag response returned eating without difficulty sensation returned extremity\n"
     ]
    }
   ],
   "source": [
    "print('Sample Transcription 1:'+df.iloc[5]['transcription']+'\\n')\n",
    "print('Sample Transcription 2:'+df.iloc[125]['transcription']+'\\n')\n",
    "print('Sample Transcription 3:'+df.iloc[1000]['transcription'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47209f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', 'abcd', 'abdomen', 'abdominal', 'abdominal pain', 'able', 'abnormal', 'abnormality', 'achieved', 'active', 'activity', 'acute', 'addition', 'additional', 'adequate', 'adhesion', 'administered', 'admission', 'admitted', 'advanced', 'age', 'ago', 'air', 'alcohol', 'alert', 'alert oriented', 'alignment', 'allergy', 'alternative', 'amplitude', 'anesthesia', 'anesthesia general', 'anesthetic', 'aneurysm', 'ankle', 'annular', 'anterior', 'anterior cervical', 'antibiotic', 'aortic', 'ap', 'apparent', 'appear', 'appearance', 'appeared', 'appears', 'appendix', 'applied', 'applied patient', 'appropriate', 'approximated', 'approximately', 'approximately cm', 'area', 'arm', 'artery', 'aspect', 'aspiration', 'assessment', 'associated', 'atrophy', 'attachment', 'attention', 'attention directed', 'axial', 'base', 'based', 'bed', 'began', 'benefit', 'benign', 'better', 'biceps', 'bid', 'bilateral', 'bilaterally', 'biopsy', 'bipolar', 'bladder', 'blade', 'ble', 'bleeding', 'block', 'blood', 'blood loss', 'blood pressure', 'blunt', 'body', 'bone', 'bony', 'bovie', 'bowel', 'bp', 'bp hr', 'bp hr rr', 'brain', 'branch', 'breath', 'brought', 'brought operating', 'brought operating room', 'bue', 'bulging', 'calcification', 'canal', 'cancer', 'capsular', 'capsule', 'cardiac', 'care', 'care taken', 'carefully', 'carotid', 'carotid artery', 'carpal', 'carpal ligament', 'carpal tunnel', 'carried', 'case', 'catheter', 'cautery', 'cavity', 'cbc', 'cc', 'cc cc', 'cecum', 'cell', 'cement', 'central', 'central canal', 'cerebral', 'cervical', 'cervical spine', 'change', 'chest', 'chest pain', 'chief', 'chief complaint', 'child', 'chronic', 'clamp', 'clear', 'clinic', 'clinical', 'close', 'closed', 'closure', 'cm', 'cn', 'collateral', 'colon', 'colonoscopy', 'common', 'compartment', 'complaint', 'complete', 'completed', 'completely', 'complex', 'complication', 'component', 'compression', 'condition', 'conduction', 'confirmed', 'consent', 'consent obtained', 'consistent', 'continue', 'continued', 'contrast', 'control', 'coord', 'copious', 'copiously', 'copiously irrigated', 'cord', 'coronal', 'coronary', 'correct', 'coumadin', 'count', 'course', 'cranial', 'created', 'csf', 'ct', 'ct scan', 'cuff', 'culture', 'current', 'currently', 'cut', 'cyst', 'cystic', 'daily', 'day', 'decompression', 'decreased', 'deep', 'defect', 'deficit', 'deformity', 'degenerative', 'degenerative change', 'degree', 'demonstrated', 'demonstrates', 'denied', 'denies', 'department', 'described', 'description', 'description procedure', 'developed', 'diabetes', 'diagnosed', 'diagnosis', 'diagnosis left', 'diagnosis right', 'diagnostic', 'diarrhea', 'died', 'diet', 'difficult', 'difficulty', 'diffuse', 'digit', 'direct', 'directed', 'disc', 'disc space', 'discectomy', 'discharge', 'discharged', 'discharged home', 'discussed', 'discussed patient', 'disease', 'disk', 'dissected', 'dissection', 'dissection carried', 'distal', 'distally', 'distress', 'divided', 'dorsal', 'dose', 'dr', 'drain', 'draped', 'draped usual', 'draped usual sterile', 'dressing', 'dressing applied', 'drift', 'drill', 'drug', 'drug use', 'dry', 'duct', 'dysarthria', 'easily', 'edema', 'edge', 'eeg', 'effect', 'effusion', 'ekg', 'elbow', 'electrocautery', 'elevated', 'emergency', 'emergency room', 'emg', 'end', 'endoscopy', 'endotracheal', 'enlarged', 'entered', 'entire', 'epidural', 'episode', 'esophageal', 'esophagus', 'estimated', 'estimated blood', 'estimated blood loss', 'etoh', 'evaluated', 'evaluation', 'event', 'evidence', 'exam', 'exam bp', 'exam bp hr', 'exam unremarkable', 'examination', 'excellent', 'experienced', 'explained', 'exposed', 'exposure', 'extended', 'extending', 'extension', 'extensive', 'extensor', 'external', 'extremity', 'extremity prepped', 'extremity prepped draped', 'eye', 'face', 'facet', 'facial', 'fall', 'family', 'family history', 'fascia', 'fashion', 'fat', 'father', 'feel', 'felt', 'female', 'femoral', 'femur', 'fetal', 'fever', 'fhx', 'field', 'fifth', 'finding', 'finger', 'fixation', 'flap', 'flexion', 'flexor', 'floor', 'flow', 'fluid', 'focal', 'foley', 'foley catheter', 'follow', 'followed', 'following', 'followup', 'foot', 'foramen', 'foraminal', 'fracture', 'fragment', 'free', 'french', 'frontal', 'function', 'fusion', 'gait', 'gallbladder', 'gen', 'gen exam', 'general', 'general anesthesia', 'general endotracheal', 'given', 'glucose', 'going', 'good', 'grade', 'graft', 'greater', 'gross', 'grossly', 'guide', 'hand', 'hardware', 'hct', 'head', 'headache', 'heart', 'heart rate', 'heent', 'height', 'help', 'hematoma', 'hemorrhage', 'hemostasis', 'hernia', 'high', 'hip', 'history', 'history patient', 'history present', 'history present illness', 'hole', 'home', 'hospital', 'hour', 'hr', 'hr rr', 'hx', 'hypertension', 'identified', 'illicit', 'illicit drug', 'illness', 'illness patient', 'image', 'imaging', 'immediately', 'impression', 'improved', 'inch', 'incised', 'incision', 'include', 'including', 'incontinence', 'increased', 'indication', 'infection', 'inferior', 'inflated', 'informed', 'informed consent', 'informed consent obtained', 'inguinal', 'initial', 'initially', 'injected', 'injection', 'injury', 'inserted', 'insertion', 'inspected', 'instrument', 'intact', 'internal', 'interrupted', 'interval', 'intraoperative', 'introduced', 'involving', 'irrigated', 'irrigation', 'iv', 'joint', 'junction', 'kidney', 'knee', 'known', 'laboratory', 'laparoscopic', 'large', 'later', 'lateral', 'lateral aspect', 'laterally', 'layer', 'le', 'left', 'left foot', 'left knee', 'left lower', 'left upper', 'leg', 'length', 'lesion', 'level', 'lidocaine', 'life', 'ligament', 'light', 'like', 'likely', 'limit', 'limited', 'line', 'liver', 'lobe', 'local', 'long', 'longitudinal', 'longus', 'loss', 'low', 'lower', 'lower extremity', 'lumbar', 'lumbar spine', 'lung', 'lymph', 'lymph node', 'make', 'male', 'management', 'manner', 'marcaine', 'marked', 'mass', 'material', 'matter', 'measure', 'measuring', 'med', 'medial', 'medial lateral', 'medially', 'median', 'medical', 'medical history', 'medication', 'memory', 'meniscus', 'metatarsal', 'mg', 'mg bid', 'mg po', 'mg qd', 'mid', 'middle', 'midline', 'mild', 'mildly', 'minimal', 'minute', 'ml', 'mm', 'mmhg', 'moderate', 'monocryl', 'month', 'morning', 'mother', 'motion', 'motor', 'motor strength', 'movement', 'mri', 'mri brain', 'mucosa', 'multiple', 'murmur', 'muscle', 'narrowing', 'nausea', 'nausea vomiting', 'near', 'neck', 'neck pain', 'need', 'needed', 'needle', 'negative', 'nerve', 'nerve root', 'neural', 'neurologic', 'neurological', 'neuropathy', 'new', 'night', 'node', 'normal', 'normal limit', 'note', 'noted', 'number', 'numbness', 'nylon', 'oblique', 'obstruction', 'obtained', 'obtained patient', 'occasional', 'office', 'old', 'onset', 'open', 'opened', 'opening', 'operating', 'operating room', 'operating room placed', 'operating table', 'operation', 'operative', 'option', 'oral', 'order', 'oriented', 'osteophyte', 'osteotomy', 'pain', 'parent', 'partial', 'passed', 'past', 'past medical', 'past medical history', 'patella', 'patellar', 'patent', 'pathology', 'patient brought', 'patient brought operating', 'patient given', 'patient placed', 'patient state', 'patient taken', 'patient taken operating', 'patient tolerated', 'patient tolerated procedure', 'patient yearold', 'pedicle', 'pelvic', 'pelvis', 'penis', 'perform', 'performed', 'person', 'person place', 'person place time', 'phalanx', 'physical', 'physical examination', 'physical therapy', 'physician', 'pin', 'place', 'place time', 'placed', 'placed supine', 'placed supine position', 'placement', 'plain', 'plan', 'plantar', 'plate', 'pleasant', 'pmh', 'po', 'point', 'polyp', 'port', 'portal', 'portion', 'position', 'positioned', 'positive', 'possible', 'post', 'posterior', 'postoperative', 'postoperative diagnosis', 'potential', 'pound', 'pp', 'preoperative', 'preoperative diagnosis', 'prepped', 'prepped draped', 'prepped draped usual', 'present', 'present illness', 'present illness patient', 'presentation', 'presented', 'pressure', 'previous', 'previously', 'primary', 'prior', 'prior presentation', 'prn', 'probably', 'problem', 'procedure', 'procedure patient', 'procedure patient brought', 'procedure patient taken', 'procedure performed', 'proceed', 'process', 'progressive', 'prominent', 'pronator', 'proper', 'prostate', 'protein', 'protrusion', 'proximal', 'proximal phalanx', 'proximally', 'pt', 'ptt', 'pulmonary', 'pulse', 'pupil', 'qd', 'quadrant', 'question', 'quite', 'radial', 'radiation', 'radiculopathy', 'range', 'range motion', 'rate', 'reapproximated', 'reason', 'reason exam', 'received', 'recent', 'recently', 'recommendation', 'recommended', 'recovery', 'recovery room', 'recovery room stable', 'rectal', 'rectum', 'recurrent', 'reduced', 'reduction', 'referred', 'reflected', 'reflex', 'reflux', 'region', 'regular', 'release', 'released', 'remained', 'remaining', 'removal', 'remove', 'removed', 'renal', 'repair', 'repeat', 'report', 'reported', 'resected', 'resection', 'residual', 'resolved', 'response', 'rest', 'result', 'retracted', 'retractor', 'return', 'returned', 'revealed', 'reveals', 'review', 'reviewed', 'rhythm', 'right', 'right foot', 'right knee', 'right lower', 'right upper', 'ring', 'risk', 'risk benefit', 'room', 'room placed', 'room stable', 'room stable condition', 'root', 'rotation', 'rotator', 'rotator cuff', 'rr', 'running', 'sac', 'sagittal', 'saline', 'satisfactory', 'saw', 'scan', 'scar', 'scissors', 'scope', 'screw', 'screw placed', 'scrotal', 'second', 'secondary', 'secured', 'sedation', 'seen', 'seizure', 'sensation', 'sensory', 'sent', 'series', 'series image', 'severe', 'sharp', 'sheath', 'short', 'shoulder', 'showed', 'shx', 'sigmoid', 'sign', 'signal', 'significant', 'silk', 'similar', 'simple', 'sinus', 'site', 'size', 'skin', 'skin closed', 'skin incision', 'sleep', 'slightly', 'small', 'social', 'social history', 'soft', 'soft tissue', 'solution', 'sound', 'space', 'specimen', 'speech', 'spinal', 'spinal cord', 'spine', 'spondylosis', 'sponge', 'stable', 'stable condition', 'standard', 'staple', 'started', 'state', 'station', 'status', 'status post', 'stenosis', 'stent', 'sterile', 'sterile dressing', 'sterile dressing applied', 'sterile fashion', 'steristrips', 'stimulation', 'stitch', 'stomach', 'stone', 'stool', 'straight', 'strength', 'stress', 'stroke', 'structure', 'study', 'subcutaneous', 'subcutaneous tissue', 'subcuticular', 'subsequently', 'suite', 'superficial', 'superior', 'supine', 'supine position', 'surface', 'surgery', 'surgical', 'surrounding', 'suture', 'swelling', 'symmetric', 'symptom', 'syndrome', 'table', 'taken', 'taken operating', 'taken operating room', 'taken recovery', 'taking', 'tear', 'technique', 'temperature', 'temporal', 'tenderness', 'tendon', 'test', 'testing', 'testis', 'therapy', 'thickening', 'thoracic', 'thought', 'tibia', 'tibial', 'time', 'tip', 'tissue', 'tobacco', 'today', 'toe', 'told', 'tolerated', 'tolerated procedure', 'tone', 'total', 'tourniquet', 'transferred', 'transverse', 'transverse carpal', 'transverse carpal ligament', 'treated', 'treatment', 'trial', 'trocar', 'tube', 'tumor', 'tunnel', 'turned', 'type', 'uihc', 'ulnar', 'ultrasound', 'unable', 'underwent', 'unit', 'unremarkable', 'upper', 'upper extremity', 'ureter', 'ureteral', 'urethra', 'urinary', 'urine', 'use', 'used', 'using', 'using vicryl', 'usual', 'usual sterile', 'usual sterile fashion', 'utilized', 'utilizing', 'valve', 'vascular', 'vein', 'velocity', 'venous', 'ventricle', 'ventricular', 'versus', 'vertebral', 'vertebral body', 'vessel', 'vicryl', 'vicryl suture', 'view', 'vision', 'visit', 'visual', 'visualization', 'visualized', 'vital', 'vital sign', 'volume', 'vomiting', 'walk', 'walking', 'wall', 'way', 'wbc', 'weakness', 'week', 'weight', 'went', 'white', 'wire', 'work', 'worse', 'wound', 'wrist', 'xray', 'xrays', 'xyz', 'year', 'year ago', 'yearold', 'yearold female', 'yearold male']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='word', stop_words='english',ngram_range=(1,3), max_df=0.75, use_idf=True, smooth_idf=True, max_features=1000)\n",
    "tfIdfMat  = vectorizer.fit_transform(df['transcription'].tolist() )\n",
    "feature_names = sorted(vectorizer.get_feature_names())\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2407d1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "tfIdfMat_reduced = pca.fit_transform(tfIdfMat.toarray())\n",
    "labels = df['medical_specialty'].tolist()\n",
    "category_list = df.medical_specialty.unique()\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfIdfMat_reduced, labels, stratify=labels,random_state=1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "132f6ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Set_Size:(929, 462)\n",
      "Test_Set_Size:(310, 462)\n"
     ]
    }
   ],
   "source": [
    "print('Train_Set_Size:'+str(X_train.shape))\n",
    "print('Test_Set_Size:'+str(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5f9e93ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty= 'elasticnet', solver= 'saga', l1_ratio=0.5, random_state=1).fit(X_train, y_train)\n",
    "y_test_pred= clf.predict(X_test)\n",
    "# y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0a489d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7483870967741936"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "29f7f8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "        Neurology       0.61      0.59      0.60        56\n",
      "          Urology       0.94      0.74      0.83        39\n",
      "        Radiology       0.59      0.65      0.62        68\n",
      "       Orthopedic       0.82      0.91      0.86        89\n",
      " Gastroenterology       0.87      0.78      0.82        58\n",
      "\n",
      "         accuracy                           0.75       310\n",
      "        macro avg       0.76      0.73      0.75       310\n",
      "     weighted avg       0.76      0.75      0.75       310\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_test_pred,labels=category_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "70646b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[45,  1,  1, 10,  1],\n",
       "       [ 0, 33,  7, 16,  0],\n",
       "       [ 0,  5, 81,  3,  0],\n",
       "       [ 1, 12, 10, 44,  1],\n",
       "       [ 6,  3,  0,  1, 29]], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = category_list\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c4426a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords  # removing all the stop words\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5220410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df['transcription']\n",
    "y= df['medical_specialty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "185b45c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []  ##  Empty corpus list, can be used to store all the text after cleaning.\n",
    "for i in range(len(X)):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = re.sub('[^a-zA-Z]', ' ', str(X.iloc[i]))\n",
    "    text = re.sub(r'\\d+', '', text) # Remove numbers\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = \" \".join(text.split()) # Remove whitespaces\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    \n",
    "    all_stopwords = stopwords.words('english')\n",
    "    text = [y for y in text if y not in all_stopwords]\n",
    "    \n",
    "    # Stemming\n",
    "    # ps = PorterStemmer()\n",
    "    # review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text if word not in set(all_stopwords)]\n",
    "\n",
    "\n",
    "    text = ' '.join(text)\n",
    "    corpus.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9bf37b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "nothing to repeat at position 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(text)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[1;32m---> 26\u001b[0m new_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranscription\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39m \u001b[43mnew_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtranscription\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4330\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4332\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1088\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1143\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1137\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   1138\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1143\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1146\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1150\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1151\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[11], line 10\u001b[0m, in \u001b[0;36mclean_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#removing square brackets\u001b[39;00m\n\u001b[0;32m      9\u001b[0m text\u001b[38;5;241m=\u001b[39mre\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[.*?]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[1;32m---> 10\u001b[0m text\u001b[38;5;241m=\u001b[39m\u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#removing hyperlink\u001b[39;00m\n\u001b[0;32m     12\u001b[0m text\u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps?://S+|www.S+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\re.py:210\u001b[0m, in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msub\u001b[39m(pattern, repl, string, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;124;03m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msub(repl, string, count)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\re.py:304\u001b[0m, in \u001b[0;36m_compile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sre_compile\u001b[38;5;241m.\u001b[39misstring(pattern):\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst argument must be string or compiled pattern\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 304\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43msre_compile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (flags \u001b[38;5;241m&\u001b[39m DEBUG):\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_cache) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m _MAXCACHE:\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;66;03m# Drop the oldest item\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\sre_compile.py:764\u001b[0m, in \u001b[0;36mcompile\u001b[1;34m(p, flags)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isstring(p):\n\u001b[0;32m    763\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m p\n\u001b[1;32m--> 764\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43msre_parse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    766\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\sre_parse.py:948\u001b[0m, in \u001b[0;36mparse\u001b[1;34m(str, flags, state)\u001b[0m\n\u001b[0;32m    945\u001b[0m state\u001b[38;5;241m.\u001b[39mstr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m\n\u001b[0;32m    947\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 948\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43m_parse_sub\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSRE_FLAG_VERBOSE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Verbose:\n\u001b[0;32m    950\u001b[0m     \u001b[38;5;66;03m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[39;00m\n\u001b[0;32m    951\u001b[0m     \u001b[38;5;66;03m# on the safe side, we'll parse the whole thing again...\u001b[39;00m\n\u001b[0;32m    952\u001b[0m     state \u001b[38;5;241m=\u001b[39m State()\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\sre_parse.py:443\u001b[0m, in \u001b[0;36m_parse_sub\u001b[1;34m(source, state, verbose, nested)\u001b[0m\n\u001b[0;32m    441\u001b[0m start \u001b[38;5;241m=\u001b[39m source\u001b[38;5;241m.\u001b[39mtell()\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 443\u001b[0m     itemsappend(\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnested\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnested\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sourcematch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\sre_parse.py:668\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[0;32m    666\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    667\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m item \u001b[38;5;129;01mor\u001b[39;00m item[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m AT:\n\u001b[1;32m--> 668\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m source\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnothing to repeat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    669\u001b[0m                        source\u001b[38;5;241m.\u001b[39mtell() \u001b[38;5;241m-\u001b[39m here \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(this))\n\u001b[0;32m    670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m item[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m _REPEATCODES:\n\u001b[0;32m    671\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m source\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiple repeat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    672\u001b[0m                        source\u001b[38;5;241m.\u001b[39mtell() \u001b[38;5;241m-\u001b[39m here \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(this))\n",
      "\u001b[1;31merror\u001b[0m: nothing to repeat at position 0"
     ]
    }
   ],
   "source": [
    "# clean_text function shown below preprocess the text data provided.\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    # lower text \n",
    "    text = text.lower()\n",
    "    #removing stop words\n",
    "    text = ' '.join([e_words for e_words in text.split(' ') if e_words not in stopwords.words('english')])\n",
    "    #removing square brackets\n",
    "    text=re.sub('[.*?]', '', text)\n",
    "    text=re.sub('+', '', text)\n",
    "    #removing hyperlink\n",
    "    text= re.sub('https?://S+|www.S+', '', text)\n",
    "    #removing puncuation\n",
    "    text=re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('n' , '', text)\n",
    "    #remove words containing numbers\n",
    "    text=re.sub('w*dw*' , '', text)\n",
    "    #tokenizer\n",
    "    text = nltk.word_tokenize(text)\n",
    "    #lemmatizer\n",
    "    wn = nltk.WordNetLemmatizer()\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "new_df[\"transcription\"]= new_df[\"transcription\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89c64b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1239, 13593)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In Machine learning algorithms mostly take numeric feature vectors as input. \n",
    "# Thus, when working with text data, need to convert each document into a numeric vector using CountVectorizer.\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "train_vectors_counts = count_vectorizer.fit_transform(df[\"transcription\"])\n",
    "\n",
    "train_vectors_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d76e00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.72\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Multinomial Naive Bayes model\n",
    "mnb = MultinomialNB()\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "print(\"Mean Accuracy: {:.2}\".format(cross_val_score(mnb, train_vectors_counts, df[\"medical_specialty\"], cv=cv).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f64ca15",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textattack'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# The below function takes text, label, and textattack augmenter \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# as input and returns a list of augmented data with their corresponding labels.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# !pip install textattack\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextattack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugmentation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EmbeddingAugmenter\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtextattack_data_augment\u001b[39m(data, target, texattack_augmenter):\n\u001b[0;32m      7\u001b[0m     aug_data \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textattack'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ill (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ill (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\ProgramData\\Anaconda3\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Harshit Singh\\\\AppData\\\\Local\\\\Temp\\\\pip-install-_18opvz1\\\\pycld2_869c7449939146ec825b7f0922998bb8\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Harshit Singh\\\\AppData\\\\Local\\\\Temp\\\\pip-install-_18opvz1\\\\pycld2_869c7449939146ec825b7f0922998bb8\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\Harshit Singh\\AppData\\Local\\Temp\\pip-wheel-souhwt0l'\n",
      "       cwd: C:\\Users\\Harshit Singh\\AppData\\Local\\Temp\\pip-install-_18opvz1\\pycld2_869c7449939146ec825b7f0922998bb8\\\n",
      "  Complete output (13 lines):\n",
      "  C:\\ProgramData\\Anaconda3\\lib\\site-packages\\setuptools\\config\\setupcfg.py:508: SetuptoolsDeprecationWarning: The license_file parameter is deprecated, use license_files instead.\n",
      "    warnings.warn(msg, warning_class)\n",
      "  running bdist_wheel\n",
      "  The [wheel] section is deprecated. Use [bdist_wheel] instead.\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-38\n",
      "  creating build\\lib.win-amd64-cpython-38\\pycld2\n",
      "  copying pycld2\\__init__.py -> build\\lib.win-amd64-cpython-38\\pycld2\n",
      "  running build_ext\n",
      "  building 'pycld2._pycld2' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for pycld2\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ill (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -ill (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\dill-0.3.6.dist-info\\\\direct_url.json'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ill (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\ProgramData\\Anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textattack\n",
      "  Using cached textattack-0.3.8-py3-none-any.whl (418 kB)\n",
      "Collecting torch!=1.8,>=1.7.0\n",
      "  Using cached torch-2.0.0-cp38-cp38-win_amd64.whl (172.3 MB)\n",
      "Collecting lru-dict\n",
      "  Using cached lru_dict-1.1.8-cp38-cp38-win_amd64.whl (12 kB)\n",
      "Collecting flair\n",
      "  Using cached flair-0.12.2-py3-none-any.whl (373 kB)\n",
      "Collecting bert-score>=0.3.5\n",
      "  Using cached bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Collecting jieba\n",
      "  Using cached jieba-0.42.1-py3-none-any.whl\n",
      "Requirement already satisfied: pandas>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from textattack) (1.4.4)\n",
      "Requirement already satisfied: click<8.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from textattack) (8.0.4)\n",
      "Requirement already satisfied: more-itertools in c:\\programdata\\anaconda3\\lib\\site-packages (from textattack) (8.12.0)\n",
      "Collecting transformers>=4.21.0\n",
      "  Using cached transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from textattack) (1.7.1)\n",
      "Collecting language-tool-python\n",
      "  Using cached language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from textattack) (4.64.1)\n",
      "Collecting OpenHowNet\n",
      "  Using cached OpenHowNet-2.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from textattack) (1.23.4)\n",
      "Collecting num2words\n",
      "  Using cached num2words-0.5.12-py3-none-any.whl (125 kB)\n",
      "Collecting terminaltables\n",
      "  Using cached terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (from textattack) (3.7)\n",
      "Collecting pycld2\n",
      "  Using cached pycld2-0.41.tar.gz (41.4 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from textattack) (1.9.3)\n",
      "Collecting editdistance\n",
      "  Using cached editdistance-0.6.2-cp38-cp38-win_amd64.whl (22 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\harshit singh\\appdata\\roaming\\python\\python38\\site-packages (from textattack) (3.4.0)\n",
      "Collecting datasets==2.4.0\n",
      "  Using cached datasets-2.4.0-py3-none-any.whl (365 kB)\n",
      "Collecting lemminflect\n",
      "  Using cached lemminflect-0.2.3-py3-none-any.whl (769 kB)\n",
      "Collecting pinyin==0.4.0\n",
      "  Using cached pinyin-0.4.0-py3-none-any.whl\n",
      "Collecting word2number\n",
      "  Using cached word2number-1.1-py3-none-any.whl\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets==2.4.0->textattack) (2.28.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets==2.4.0->textattack) (2022.11.0)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Using cached pyarrow-11.0.0-cp38-cp38-win_amd64.whl (20.6 MB)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets==2.4.0->textattack) (0.13.3)\n",
      "Collecting xxhash\n",
      "  Using cached xxhash-3.2.0-cp38-cp38-win_amd64.whl (30 kB)\n",
      "Collecting multiprocess\n",
      "  Using cached multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "Collecting responses<0.19\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting dill<0.3.6\n",
      "  Using cached dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets==2.4.0->textattack) (22.0)\n",
      "Requirement already satisfied: aiohttp in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets==2.4.0->textattack) (3.7.4.post0)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (from bert-score>=0.3.5->textattack) (3.6.2)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click<8.1.0->textattack) (0.4.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.0.1->textattack) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.0.1->textattack) (2.8.2)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch!=1.8,>=1.7.0->textattack) (1.11.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch!=1.8,>=1.7.0->textattack) (4.4.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch!=1.8,>=1.7.0->textattack) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch!=1.8,>=1.7.0->textattack) (2.11.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers>=4.21.0->textattack) (2022.7.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers>=4.21.0->textattack) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers>=4.21.0->textattack) (0.13.2)\n",
      "Collecting tabulate\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting hyperopt>=0.2.7\n",
      "  Using cached hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from flair->textattack) (1.1.3)\n",
      "Collecting deprecated>=1.2.4\n",
      "  Using cached Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting transformer-smaller-training-vocab>=0.2.1\n",
      "  Using cached transformer_smaller_training_vocab-0.2.3-py3-none-any.whl (12 kB)\n",
      "Collecting gdown==4.4.0\n",
      "  Using cached gdown-4.4.0-py3-none-any.whl\n",
      "Collecting langdetect\n",
      "  Using cached langdetect-1.0.9-py3-none-any.whl\n",
      "Collecting pptree\n",
      "  Using cached pptree-3.1-py3-none-any.whl\n",
      "Requirement already satisfied: lxml in c:\\programdata\\anaconda3\\lib\\site-packages (from flair->textattack) (4.9.1)\n",
      "Collecting mpld3==0.3\n",
      "  Using cached mpld3-0.3-py3-none-any.whl\n",
      "Collecting bpemb>=0.3.2\n",
      "  Using cached bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
      "Collecting sqlitedict>=1.6.0\n",
      "  Using cached sqlitedict-2.1.0-py3-none-any.whl\n",
      "Collecting janome\n",
      "  Using cached Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
      "Requirement already satisfied: gensim>=3.8.0 in c:\\users\\harshit singh\\appdata\\roaming\\python\\python38\\site-packages (from flair->textattack) (4.1.2)\n",
      "Collecting conllu>=4.0\n",
      "  Using cached conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting wikipedia-api\n",
      "  Using cached Wikipedia_API-0.5.8-py3-none-any.whl (13 kB)\n",
      "Collecting pytorch-revgrad\n",
      "  Using cached pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
      "Collecting segtok>=1.5.7\n",
      "  Using cached segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Collecting boto3\n",
      "  Using cached boto3-1.26.105-py3-none-any.whl (135 kB)\n",
      "Collecting ftfy\n",
      "  Using cached ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from gdown==4.4.0->flair->textattack) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from gdown==4.4.0->flair->textattack) (4.11.1)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk->textattack) (1.1.1)\n",
      "Collecting docopt>=0.6.2\n",
      "  Using cached docopt-0.6.2-py2.py3-none-any.whl\n",
      "Collecting anytree\n",
      "  Using cached anytree-2.8.0-py2.py3-none-any.whl (41 kB)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from OpenHowNet->textattack) (65.5.0)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.97-cp38-cp38-win_amd64.whl (1.1 MB)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from deprecated>=1.2.4->flair->textattack) (1.14.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.4.0->textattack) (1.6.3)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.4.0->textattack) (3.0.1)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.4.0->textattack) (4.0.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.4.0->textattack) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.4.0->textattack) (5.1.0)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\harshit singh\\appdata\\roaming\\python\\python38\\site-packages (from gensim>=3.8.0->flair->textattack) (0.29.23)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim>=3.8.0->flair->textattack) (5.2.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\programdata\\anaconda3\\lib\\site-packages (from hyperopt>=0.2.7->flair->textattack) (2.0.0)\n",
      "Collecting py4j\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Requirement already satisfied: future in c:\\programdata\\anaconda3\\lib\\site-packages (from hyperopt>=0.2.7->flair->textattack) (0.18.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->bert-score>=0.3.5->textattack) (9.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->bert-score>=0.3.5->textattack) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->bert-score>=0.3.5->textattack) (4.25.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->bert-score>=0.3.5->textattack) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.0.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.4.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets==2.4.0->textattack) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets==2.4.0->textattack) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets==2.4.0->textattack) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets==2.4.0->textattack) (2.0.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->flair->textattack) (2.2.0)\n",
      "Requirement already satisfied: protobuf<=3.20.2 in c:\\users\\harshit singh\\appdata\\roaming\\python\\python38\\site-packages (from transformers>=4.21.0->textattack) (3.19.1)\n",
      "Collecting botocore<1.30.0,>=1.29.105\n",
      "  Using cached botocore-1.29.105-py3-none-any.whl (10.6 MB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0\n",
      "  Using cached s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->flair->textattack) (1.0.1)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from ftfy->flair->textattack) (0.2.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch!=1.8,>=1.7.0->textattack) (2.0.1)\n",
      "Collecting multiprocess\n",
      "  Using cached multiprocess-0.70.13-py38-none-any.whl (131 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch!=1.8,>=1.7.0->textattack) (1.2.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown==4.4.0->flair->textattack) (2.3.2.post1)\n",
      "Building wheels for collected packages: pycld2\n",
      "  Building wheel for pycld2 (setup.py): started\n",
      "  Building wheel for pycld2 (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for pycld2\n",
      "Failed to build pycld2\n",
      "Installing collected packages: dill, xxhash, transformers, torch, responses, pyarrow, multiprocess, botocore, sentencepiece, s3transfer, py4j, datasets, wikipedia-api, transformer-smaller-training-vocab, tabulate, sqlitedict, segtok, pytorch-revgrad, pptree, mpld3, langdetect, janome, hyperopt, gdown, ftfy, docopt, deprecated, conllu, bpemb, boto3, anytree, word2number, terminaltables, pycld2, pinyin, OpenHowNet, num2words, lru-dict, lemminflect, language-tool-python, jieba, flair, editdistance, bert-score, textattack\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.6\n",
      "    Uninstalling dill-0.3.6:\n"
     ]
    }
   ],
   "source": [
    "# The below function takes text, label, and textattack augmenter \n",
    "# as input and returns a list of augmented data with their corresponding labels.\n",
    "# !pip install textattack\n",
    "from textattack.augmentation import EmbeddingAugmenter\n",
    "def textattack_data_augment(data, target, texattack_augmenter):\n",
    "\n",
    "    aug_data = []\n",
    "\n",
    "    aug_label = []\n",
    "\n",
    "    for text, label in zip(data, target):\n",
    "\n",
    "        if random.randint(0,2) != 1:\n",
    "\n",
    "            aug_data.append(text)\n",
    "\n",
    "            aug_label.append(label)\n",
    "\n",
    "            continue\n",
    "\n",
    "        aug_list = texattack_augmenter.augment(text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        aug_data.append(text)\n",
    "\n",
    "        aug_label.append(label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        aug_data.extend(aug_list)\n",
    "\n",
    "        aug_label.extend([label]*len(aug_list))\n",
    "\n",
    "    return aug_data, aug_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd01094b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EmbeddingAugmenter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Here, for Data Augmentation we are configuring EmbeddedAumenter which will further pass to the function listed above.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m embed_aug \u001b[38;5;241m=\u001b[39m \u001b[43mEmbeddingAugmenter\u001b[49m(pct_words_to_swap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, transformations_per_example\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m aug_data, aug_lable \u001b[38;5;241m=\u001b[39m textattack_data_augment(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranscription\u001b[39m\u001b[38;5;124m\"\u001b[39m], df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedical_specialty\u001b[39m\u001b[38;5;124m\"\u001b[39m], embed_aug)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'EmbeddingAugmenter' is not defined"
     ]
    }
   ],
   "source": [
    "# Here, for Data Augmentation we are configuring EmbeddedAumenter which will further pass to the function listed above.\n",
    "embed_aug = EmbeddingAugmenter(pct_words_to_swap=0.1, transformations_per_example=1)\n",
    "aug_data, aug_lable = textattack_data_augment(df[\"transcription\"], df[\"medical_specialty\"], embed_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9554663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are cleaning the text using text cleaning function and countvectorizer\n",
    "clean_aug_data = [text_cleaning(txt) for txt in aug_data]\n",
    "count_vect = CountVectorizer()\n",
    "aug_data_counts = count_vect.fit_transform(clean_aug_data)\n",
    "aug_data_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ca81f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Accuracy: {:.2}\".format(cross_val_score(mnb, aug_data_counts, aug_lable, cv=cv).mean()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
